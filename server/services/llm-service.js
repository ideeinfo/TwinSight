/**
 * é€šç”¨ LLM æœåŠ¡
 * ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼è°ƒç”¨é…ç½®çš„ LLM æä¾›å•†
 */

import { getConfig } from './config-service.js';

/**
 * è·å– LLM é…ç½®
 */
async function getLLMConfig() {
    const provider = await getConfig('LLM_PROVIDER', 'gemini');
    const apiKey = await getConfig('LLM_API_KEY', '');
    const baseUrl = await getConfig('LLM_BASE_URL', 'https://generativelanguage.googleapis.com/v1beta/openai/');
    const model = await getConfig('LLM_MODEL', 'gemini-2.0-flash');

    return { provider, apiKey, baseUrl, model };
}

/**
 * è°ƒç”¨ LLM è¿›è¡Œå¯¹è¯
 * @param {string} systemPrompt - ç³»ç»Ÿæç¤ºè¯
 * @param {string} userMessage - ç”¨æˆ·æ¶ˆæ¯
 * @param {Object} options - å¯é€‰é…ç½®
 * @returns {Promise<{success: boolean, content?: string, error?: string}>}
 */
export async function chat(systemPrompt, userMessage, options = {}) {
    try {
        const config = await getLLMConfig();

        if (!config.apiKey) {
            console.error('âŒ LLM_API_KEY æœªé…ç½®ï¼Œè¯·åœ¨ç³»ç»Ÿé…ç½®ä¸­è®¾ç½®');
            return { success: false, error: 'LLM API Key æœªé…ç½®ï¼Œè¯·åœ¨ AI è®¾ç½®ä¸­é…ç½®' };
        }

        const chatUrl = config.baseUrl.replace(/\/$/, '') + '/chat/completions';

        console.log(`ğŸ¤– è°ƒç”¨ LLM API: ${config.provider}, æ¨¡å‹: ${config.model}`);

        const messages = [];
        if (systemPrompt) {
            messages.push({ role: 'system', content: systemPrompt });
        }
        messages.push({ role: 'user', content: userMessage });

        const response = await fetch(chatUrl, {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${config.apiKey}`,
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model: config.model,
                messages,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens || 2048
            })
        });

        if (!response.ok) {
            const errorText = await response.text();
            console.error('âŒ LLM API è°ƒç”¨å¤±è´¥:', response.status, errorText);
            return { success: false, error: `LLM API error: ${response.status}` };
        }

        const result = await response.json();
        const content = result.choices?.[0]?.message?.content || '';

        console.log(`âœ… LLM å“åº”å®Œæˆï¼Œé•¿åº¦: ${content.length} å­—ç¬¦`);

        return { success: true, content };
    } catch (error) {
        console.error('âŒ LLM API è°ƒç”¨å¼‚å¸¸:', error.message);
        return { success: false, error: error.message };
    }
}

/**
 * è°ƒç”¨ LLM è¿›è¡Œæ¸©åº¦å¼‚å¸¸åˆ†æ
 * @param {Object} alertData - æŠ¥è­¦æ•°æ®
 */
export async function analyzeTemperatureAlert(alertData) {
    const { roomCode, roomName, temperature, threshold, alertType = 'high' } = alertData;

    const isHighTemp = alertType === 'high';
    const alertTypeText = isHighTemp ? 'é«˜æ¸©' : 'ä½æ¸©';

    // æ„å»º Prompt
    const prompt = `ä½ æ˜¯ä¸€ä¸ªå»ºç­‘è®¾å¤‡è¿ç»´ä¸“å®¶ã€‚ç³»ç»Ÿæ£€æµ‹åˆ°ä»¥ä¸‹${alertTypeText}å¼‚å¸¸ï¼š

ã€æŠ¥è­¦ä¿¡æ¯ã€‘
- ä½ç½®: ${roomName} (${roomCode})
- å½“å‰æ¸©åº¦: ${temperature}Â°C
- ${isHighTemp ? 'é«˜æ¸©é˜ˆå€¼' : 'ä½æ¸©é˜ˆå€¼'}: ${threshold}Â°C
- æŠ¥è­¦ç±»å‹: ${alertTypeText}
- æ—¶é—´: ${new Date().toISOString()}

è¯·é’ˆå¯¹${alertTypeText}å¼‚å¸¸æä¾›ä¸“ä¸šåˆ†ææŠ¥å‘Šï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

## åŸå› åˆ†æ
[åˆ—å‡º3-5ä¸ªå¯èƒ½å¯¼è‡´${alertTypeText}å¼‚å¸¸çš„åŸå› ï¼Œ${isHighTemp ? 'ä¾‹å¦‚ï¼šå†·å´ç³»ç»Ÿæ•…éšœã€è¿‡è½½è¿è¡Œã€æ•£çƒ­ä¸è‰¯ç­‰' : 'ä¾‹å¦‚ï¼šä¾›æš–ç³»ç»Ÿæ•…éšœã€ä¿æ¸©å±‚æŸåã€é—¨çª—å¯†å°ä¸è‰¯ç­‰'}]

## å¤„ç½®æ–¹æ¡ˆ
[åˆ†æ­¥éª¤åˆ—å‡ºé’ˆå¯¹${alertTypeText}çš„å»ºè®®å¤„ç½®æªæ–½]

## é¢„é˜²å»ºè®®
[åˆ—å‡ºé¢„é˜²æ­¤ç±»${alertTypeText}é—®é¢˜çš„æªæ–½]

æ³¨æ„ï¼šç›´æ¥ç»™å‡ºä¸“ä¸šå»ºè®®ï¼Œä¸è¦è¿½é—®æ›´å¤šä¿¡æ¯ã€‚ç”¨ä¸­æ–‡å›ç­”ã€‚`;

    const result = await chat('', prompt);

    if (!result.success) {
        return result;
    }

    return {
        success: true,
        analysis: result.content,
        alert: {
            roomCode,
            roomName,
            temperature,
            threshold,
            alertType,
            alertTypeText
        }
    };
}

export default {
    chat,
    analyzeTemperatureAlert,
};
